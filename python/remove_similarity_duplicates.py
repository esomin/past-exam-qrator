#!/usr/bin/env python3
"""
TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ëª¨ë“ˆ
answers_unique.json íŒŒì¼ì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹µë³€ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
"""

import json
import os
import re
import math
from typing import List, Dict, Any, Tuple
from pathlib import Path
from collections import Counter


class SimilarityDeduplicator:
    """TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° í´ë˜ìŠ¤"""
    
    def __init__(self, input_file: str = "data/answers.json", output_dir: str = "data", threshold: float = 0.8):
        self.input_file = input_file
        self.output_dir = output_dir
        self.threshold = threshold
        self.ensure_output_dir()
    
    def ensure_output_dir(self) -> None:
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def load_answers(self) -> List[Dict[str, Any]]:
        """answers_unique.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“ Loaded {len(data)} answers from {self.input_file}")
            return data
        except FileNotFoundError:
            print(f"âŒ File not found: {self.input_file}")
            raise
        except json.JSONDecodeError as e:
            print(f"âŒ JSON decode error: {e}")
            raise
    
    def save_json_file(self, data: Any, filename: str) -> None:
        """JSON ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved: {filepath}")
    
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        if not text:
            return []
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìˆ«ì ì •ê·œí™”
        text = re.sub(r'\d{4}ë…„', 'YEARë…„', text)
        text = re.sub(r'\d+%', 'PERCENT', text)
        text = re.sub(r'\d+ë²ˆ', 'NUMBERë²ˆ', text)
        text = re.sub(r'\d+\.', 'NUMBER.', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ê·œí™”
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ì¡°ì‚¬, ì–´ë¯¸ ê°„ì†Œí™” (ê¸°ë³¸ì ì¸ ê²ƒë“¤ë§Œ)
        text = re.sub(r'ì…ë‹ˆë‹¤$', 'ì´ë‹¤', text)
        text = re.sub(r'ìŠµë‹ˆë‹¤$', 'ë‹¤', text)
        text = re.sub(r'ì—ì„œ$', 'ì—', text)
        
        # í† í°í™” (ê³µë°± ê¸°ì¤€)
        tokens = text.strip().split()
        
        # ê¸¸ì´ê°€ 1ì¸ í† í° ì œê±° (ì¡°ì‚¬ ë“±)
        tokens = [token for token in tokens if len(token) > 1]
        
        return tokens
    
    def calculate_tf(self, tokens: List[str]) -> Dict[str, float]:
        """ë‹¨ì–´ ë¹ˆë„(TF) ê³„ì‚°"""
        if not tokens:
            return {}
        
        tf_dict = Counter(tokens)
        total_words = len(tokens)
        
        # ì •ê·œí™”
        for word in tf_dict:
            tf_dict[word] = tf_dict[word] / total_words
        
        return dict(tf_dict)
    
    def calculate_idf(self, documents: List[List[str]]) -> Dict[str, float]:
        """ì—­ë¬¸ì„œ ë¹ˆë„(IDF) ê³„ì‚°"""
        if not documents:
            return {}
        
        N = len(documents)
        all_words = set(word for doc in documents for word in doc)
        idf_dict = {}
        
        for word in all_words:
            containing_docs = sum(1 for doc in documents if word in doc)
            if containing_docs > 0:
                idf_dict[word] = math.log(N / containing_docs)
            else:
                idf_dict[word] = 0
        
        return idf_dict
    
    def create_tfidf_vector(self, tf_dict: Dict[str, float], idf_dict: Dict[str, float], vocabulary: List[str]) -> List[float]:
        """TF-IDF ë²¡í„° ìƒì„±"""
        vector = []
        for word in vocabulary:
            tf = tf_dict.get(word, 0)
            idf = idf_dict.get(word, 0)
            vector.append(tf * idf)
        return vector
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def should_skip_comparison(self, answer1: str, answer2: str, tokens1: List[str], tokens2: List[str]) -> bool:
        """ë¹„êµë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •í•˜ëŠ” ì‚¬ì „ í•„í„°ë§"""
        # ê¸¸ì´ ì°¨ì´ê°€ 3ë°° ì´ìƒì´ë©´ ê±´ë„ˆë›°ê¸°
        len1, len2 = len(answer1), len(answer2)
        if len1 == 0 or len2 == 0:
            return True
        
        ratio = max(len1, len2) / min(len1, len2)
        if ratio > 3.0:
            return True
        
        # í† í° ìˆ˜ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ê±´ë„ˆë›°ê¸°
        token_len1, token_len2 = len(tokens1), len(tokens2)
        if token_len1 == 0 or token_len2 == 0:
            return True
        
        token_ratio = max(token_len1, token_len2) / min(token_len1, token_len2)
        if token_ratio > 2.5:
            return True
        
        # ê³µí†µ í† í°ì´ 30% ë¯¸ë§Œì´ë©´ ê±´ë„ˆë›°ê¸°
        common_tokens = set(tokens1) & set(tokens2)
        total_unique_tokens = len(set(tokens1) | set(tokens2))
        if total_unique_tokens > 0:
            common_ratio = len(common_tokens) / total_unique_tokens
            if common_ratio < 0.3:
                return True
        
        return False
    
    def find_similar_groups(self, answers: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ìœ ì‚¬í•œ ë‹µë³€ë“¤ì„ ì°¾ì•„ì„œ ê·¸ë£¹í™”"""
        print(f"ğŸ” Finding similar answers with threshold {self.threshold}...")
        
        # ì „ì²˜ë¦¬
        answer_texts = [answer.get('answer', '') for answer in answers]
        processed_texts = [self.preprocess_text(text) for text in answer_texts]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_indices = [i for i, tokens in enumerate(processed_texts) if tokens]
        valid_answers = [answers[i] for i in valid_indices]
        valid_processed = [processed_texts[i] for i in valid_indices]
        
        if len(valid_answers) <= 1:
            return valid_answers, []
        
        print(f"ğŸ“Š Processing {len(valid_answers)} valid answers...")
        
        # TF-IDF ê³„ì‚°
        print("ğŸ“Š Calculating TF-IDF vectors...")
        idf_dict = self.calculate_idf(valid_processed)
        vocabulary = sorted(idf_dict.keys())
        print(f"ğŸ“š Vocabulary size: {len(vocabulary)}")
        
        # ê° ë‹µë³€ì˜ TF-IDF ë²¡í„° ìƒì„±
        tfidf_vectors = []
        for i, tokens in enumerate(valid_processed):
            if i % 500 == 0:
                print(f"   Vectorizing... {i}/{len(valid_processed)}")
            tf_dict = self.calculate_tf(tokens)
            vector = self.create_tfidf_vector(tf_dict, idf_dict, vocabulary)
            tfidf_vectors.append(vector)
        
        # ìœ ì‚¬ë„ ê³„ì‚° ë° ê·¸ë£¹í™”
        print("ğŸ”— Grouping similar answers...")
        processed_indices = set()
        unique_answers = []
        similar_groups = []
        total_comparisons = 0
        
        for i in range(len(valid_answers)):
            if i in processed_indices:
                continue
            
            if i % 100 == 0:
                print(f"   Processing... {i}/{len(valid_answers)} ({len(unique_answers)} groups found)")
            
            current_group = [i]
            current_similarities = []
            
            for j in range(i + 1, len(valid_answers)):
                if j in processed_indices:
                    continue
                
                # ì‚¬ì „ í•„í„°ë§
                if self.should_skip_comparison(
                    answer_texts[valid_indices[i]], 
                    answer_texts[valid_indices[j]],
                    valid_processed[i],
                    valid_processed[j]
                ):
                    continue
                
                total_comparisons += 1
                similarity = self.cosine_similarity(tfidf_vectors[i], tfidf_vectors[j])
                
                if similarity >= self.threshold:
                    current_group.append(j)
                    current_similarities.append(similarity)
                    processed_indices.add(j)
            
            # ëŒ€í‘œ ë‹µë³€ ì„ íƒ (ê°€ì¥ ê¸´ ë‹µë³€ì„ ì„ íƒ)
            representative_idx = max(current_group, key=lambda idx: len(valid_answers[idx].get('answer', '')))
            representative = valid_answers[representative_idx].copy()
            
            if len(current_group) > 1:
                # ìœ ì‚¬í•œ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°
                representative['similarityCount'] = len(current_group)
                representative['avgSimilarity'] = sum(current_similarities) / len(current_similarities) if current_similarities else 0
                
                # ì œê±°ëœ ë‹µë³€ë“¤ ì •ë³´
                removed_answers = [valid_answers[idx] for idx in current_group if idx != representative_idx]
                similar_groups.append({
                    'representativeId': representative.get('id'),
                    'representativeAnswer': representative.get('answer', ''),
                    'similarityCount': len(current_group),
                    'avgSimilarity': representative['avgSimilarity'],
                    'removedAnswers': removed_answers
                })
            
            unique_answers.append(representative)
            processed_indices.add(i)
        
        # similarityCount í° ìˆœìœ¼ë¡œ ì •ë ¬
        unique_answers.sort(key=lambda x: x.get('similarityCount', 0), reverse=True)
        
        print(f"ğŸ“Š Total comparisons made: {total_comparisons:,}")
        return unique_answers, similar_groups
    
    def process_and_save(self) -> None:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ë° íŒŒì¼ ì €ì¥"""
        print('ğŸš€ Starting similarity-based deduplication...')
        
        # ë°ì´í„° ë¡œë“œ
        answers = self.load_answers()
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        unique_answers, similar_groups = self.find_similar_groups(answers)
        
        # ê²°ê³¼ ì €ì¥
        self.save_json_file(unique_answers, 'answers_similarity_unique.json')
        self.save_json_file(similar_groups, 'answers_similarity_removed.json')
        
        # í†µê³„ ì¶œë ¥
        total_removed = sum(len(group['removedAnswers']) for group in similar_groups)
        removal_rate = (total_removed / len(answers)) * 100 if len(answers) > 0 else 0
        similarity_group_rate = (len(similar_groups) / len(unique_answers)) * 100 if len(unique_answers) > 0 else 0
        
        print(f'âœ¨ Original answers: {len(answers)}')
        print(f'âœ¨ Similarity-unique answers: {len(unique_answers)}')
        print(f'âœ¨ Removed by similarity: {total_removed}')
        print(f'âœ¨ Similarity groups: {len(similar_groups)}')
        print(f'ğŸ“Š Similarity removal rate: {removal_rate:.2f}%')
        print(f'ğŸ“Š Similarity group rate: {similarity_group_rate:.2f}%')
        
        # ì„ê³„ê°’ë³„ í†µê³„
        if similar_groups:
            avg_similarity = sum(group['avgSimilarity'] for group in similar_groups) / len(similar_groups)
            print(f'ğŸ“Š Average similarity in groups: {avg_similarity:.3f}')
    
    def run(self) -> None:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì‹¤í–‰"""
        print('ğŸš€ Starting Similarity-based Duplicate Removal')
        
        try:
            self.process_and_save()
            print('âœ… Similarity-based deduplication completed successfully!')
            
        except Exception as error:
            print(f'âŒ Similarity-based deduplication failed: {error}')
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°')
    parser.add_argument('--input', '-i', default='data/answers_unique.json', help='ì…ë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', default='data', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--threshold', '-t', type=float, default=0.8, help='ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)')
    
    args = parser.parse_args()
    
    deduplicator = SimilarityDeduplicator(
        input_file=args.input,
        output_dir=args.output,
        threshold=args.threshold
    )
    deduplicator.run()


if __name__ == "__main__":
    main()