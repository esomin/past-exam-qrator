#!/usr/bin/env python3
"""
TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ëª¨ë“ˆ
answers.json íŒŒì¼ì—ì„œ ì˜ë¯¸ì—†ëŠ” ë‹µë³€ì„ í•„í„°ë§í•œ í›„ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹µë³€ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
"""

import json
import os
import re
import math
import sys
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple
from pathlib import Path
from collections import Counter
from filter_answers import AnswerFilter


class LogCapture:
    """ë¡œê·¸ë¥¼ ìº¡ì²˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, log_file: str):
        self.log_file = log_file
        self.logs = []
        self.original_stdout = sys.stdout
        
    def write(self, text: str):
        """stdoutì— ì“°ì—¬ì§€ëŠ” ë‚´ìš©ì„ ìº¡ì²˜"""
        self.original_stdout.write(text)
        self.logs.append(text)
        
    def flush(self):
        """flush ë©”ì„œë“œ"""
        self.original_stdout.flush()
        
    def save_logs(self):
        """ìº¡ì²˜ëœ ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(f"=== Similarity Deduplication Log - {timestamp} ===\n\n")
            f.write(''.join(self.logs))
        print(f"ğŸ“ Log saved: {self.log_file}")


class SimilarityDeduplicator:
    """TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° í´ë˜ìŠ¤"""
    
    def __init__(self, input_file: str = "data/answers.json", output_dir: str = "data", threshold: float = 0.8):
        self.input_file = input_file
        self.output_dir = output_dir
        self.threshold = threshold
        self.ensure_output_dir()
        
        # ë¡œê·¸ ìº¡ì²˜ ì„¤ì •
        self.log_capture = LogCapture(os.path.join(output_dir, 'similarity_deduplication.log'))
        
        # ë‹µë³€ í•„í„°ë§ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.answer_filter = AnswerFilter(input_file=input_file, output_dir=output_dir)
    
    def ensure_output_dir(self) -> None:
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def load_answers(self) -> List[Dict[str, Any]]:
        """answers.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“ Loaded {len(data)} answers from {self.input_file}")
            return data
        except FileNotFoundError:
            print(f"âŒ File not found: {self.input_file}")
            raise
        except json.JSONDecodeError as e:
            print(f"âŒ JSON decode error: {e}")
            raise
    
    def save_json_file(self, data: Any, filename: str) -> None:
        """JSON ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved: {filepath}")
    
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        if not text:
            return []
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìˆ«ì ì •ê·œí™”
        text = re.sub(r'\d{4}ë…„', 'YEARë…„', text)
        text = re.sub(r'\d+%', 'PERCENT', text)
        text = re.sub(r'\d+ë²ˆ', 'NUMBERë²ˆ', text)
        text = re.sub(r'\d+\.', 'NUMBER.', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ê·œí™”
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ì¡°ì‚¬, ì–´ë¯¸ ê°„ì†Œí™” (ê¸°ë³¸ì ì¸ ê²ƒë“¤ë§Œ)
        text = re.sub(r'ì…ë‹ˆë‹¤$', 'ì´ë‹¤', text)
        text = re.sub(r'ìŠµë‹ˆë‹¤$', 'ë‹¤', text)
        text = re.sub(r'ì—ì„œ$', 'ì—', text)
        
        # í† í°í™” (ê³µë°± ê¸°ì¤€)
        tokens = text.strip().split()
        
        # ê¸¸ì´ê°€ 1ì¸ í† í° ì œê±° (ì¡°ì‚¬ ë“±)
        tokens = [token for token in tokens if len(token) > 1]
        
        return tokens
    
    def calculate_tf(self, tokens: List[str]) -> Dict[str, float]:
        """ë‹¨ì–´ ë¹ˆë„(TF) ê³„ì‚°"""
        if not tokens:
            return {}
        
        tf_dict = Counter(tokens)
        total_words = len(tokens)
        
        # ì •ê·œí™”
        for word in tf_dict:
            tf_dict[word] = tf_dict[word] / total_words
        
        return dict(tf_dict)
    
    def calculate_idf(self, documents: List[List[str]]) -> Dict[str, float]:
        """ì—­ë¬¸ì„œ ë¹ˆë„(IDF) ê³„ì‚°"""
        if not documents:
            return {}
        
        N = len(documents)
        all_words = set(word for doc in documents for word in doc)
        idf_dict = {}
        
        for word in all_words:
            containing_docs = sum(1 for doc in documents if word in doc)
            if containing_docs > 0:
                idf_dict[word] = math.log(N / containing_docs)
            else:
                idf_dict[word] = 0
        
        return idf_dict
    
    def create_tfidf_vector(self, tf_dict: Dict[str, float], idf_dict: Dict[str, float], vocabulary: List[str]) -> List[float]:
        """TF-IDF ë²¡í„° ìƒì„±"""
        vector = []
        for word in vocabulary:
            tf = tf_dict.get(word, 0)
            idf = idf_dict.get(word, 0)
            vector.append(tf * idf)
        return vector
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def should_skip_comparison(self, answer1: str, answer2: str, tokens1: List[str], tokens2: List[str]) -> bool:
        """ë¹„êµë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •í•˜ëŠ” ì‚¬ì „ í•„í„°ë§"""
        # ê¸¸ì´ ì°¨ì´ê°€ 3ë°° ì´ìƒì´ë©´ ê±´ë„ˆë›°ê¸°
        len1, len2 = len(answer1), len(answer2)
        if len1 == 0 or len2 == 0:
            return True
        
        ratio = max(len1, len2) / min(len1, len2)
        if ratio > 3.0:
            return True
        
        # í† í° ìˆ˜ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ê±´ë„ˆë›°ê¸°
        token_len1, token_len2 = len(tokens1), len(tokens2)
        if token_len1 == 0 or token_len2 == 0:
            return True
        
        token_ratio = max(token_len1, token_len2) / min(token_len1, token_len2)
        if token_ratio > 2.5:
            return True
        
        # ê³µí†µ í† í°ì´ 30% ë¯¸ë§Œì´ë©´ ê±´ë„ˆë›°ê¸°
        common_tokens = set(tokens1) & set(tokens2)
        total_unique_tokens = len(set(tokens1) | set(tokens2))
        if total_unique_tokens > 0:
            common_ratio = len(common_tokens) / total_unique_tokens
            if common_ratio < 0.3:
                return True
        
        return False
    
    def find_similar_groups(self, answers: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ìœ ì‚¬í•œ ë‹µë³€ë“¤ì„ ì°¾ì•„ì„œ ê·¸ë£¹í™”"""
        start_time = time.time()
        print(f"ğŸ” Finding similar answers with threshold {self.threshold}...")
        
        # ì „ì²˜ë¦¬
        preprocess_start = time.time()
        answer_texts = [answer.get('answer', '') for answer in answers]
        processed_texts = [self.preprocess_text(text) for text in answer_texts]
        preprocess_time = time.time() - preprocess_start
        print(f"â±ï¸ Text preprocessing completed in {preprocess_time:.2f} seconds")
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_indices = [i for i, tokens in enumerate(processed_texts) if tokens]
        valid_answers = [answers[i] for i in valid_indices]
        valid_processed = [processed_texts[i] for i in valid_indices]
        
        if len(valid_answers) <= 1:
            return valid_answers, []
        
        print(f"ğŸ“Š Processing {len(valid_answers)} valid answers...")
        
        # TF-IDF ê³„ì‚°
        tfidf_start = time.time()
        print("ğŸ“Š Calculating TF-IDF vectors...")
        idf_dict = self.calculate_idf(valid_processed)
        vocabulary = sorted(idf_dict.keys())
        print(f"ğŸ“š Vocabulary size: {len(vocabulary)}")
        
        # ê° ë‹µë³€ì˜ TF-IDF ë²¡í„° ìƒì„±
        tfidf_vectors = []
        for i, tokens in enumerate(valid_processed):
            if i % 500 == 0:
                print(f"   Vectorizing... {i}/{len(valid_processed)}")
            tf_dict = self.calculate_tf(tokens)
            vector = self.create_tfidf_vector(tf_dict, idf_dict, vocabulary)
            tfidf_vectors.append(vector)
        
        tfidf_time = time.time() - tfidf_start
        print(f"â±ï¸ TF-IDF vectorization completed in {tfidf_time:.2f} seconds")
        
        # ìœ ì‚¬ë„ ê³„ì‚° ë° ê·¸ë£¹í™”
        similarity_start = time.time()
        print("ğŸ”— Grouping similar answers...")
        processed_indices = set()
        unique_answers = []
        similar_groups = []
        total_comparisons = 0
        
        for i in range(len(valid_answers)):
            if i in processed_indices:
                continue
            
            if i % 100 == 0:
                print(f"   Processing... {i}/{len(valid_answers)} ({len(unique_answers)} groups found)")
            
            current_group = [i]
            current_similarities = []
            
            for j in range(i + 1, len(valid_answers)):
                if j in processed_indices:
                    continue
                
                # ì‚¬ì „ í•„í„°ë§
                if self.should_skip_comparison(
                    answer_texts[valid_indices[i]], 
                    answer_texts[valid_indices[j]],
                    valid_processed[i],
                    valid_processed[j]
                ):
                    continue
                
                total_comparisons += 1
                similarity = self.cosine_similarity(tfidf_vectors[i], tfidf_vectors[j])
                
                if similarity >= self.threshold:
                    current_group.append(j)
                    current_similarities.append(similarity)
                    processed_indices.add(j)
            
            # ëŒ€í‘œ ë‹µë³€ ì„ íƒ (ê°€ì¥ ê¸´ ë‹µë³€ì„ ì„ íƒ)
            representative_idx = max(current_group, key=lambda idx: len(valid_answers[idx].get('answer', '')))
            representative = valid_answers[representative_idx].copy()
            
            if len(current_group) > 1:
                # ìœ ì‚¬í•œ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°
                representative['similarityCount'] = len(current_group)
                representative['avgSimilarity'] = sum(current_similarities) / len(current_similarities) if current_similarities else 0
                
                # ì œê±°ëœ ë‹µë³€ë“¤ ì •ë³´ (question ì†ì„± í¬í•¨)
                removed_answers = []
                for idx in current_group:
                    if idx != representative_idx:
                        removed_answer = valid_answers[idx].copy()
                        # question ì†ì„±ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                        if 'question' not in removed_answer:
                            removed_answer['question'] = ''
                        removed_answers.append(removed_answer)
                
                similar_groups.append({
                    'representativeId': representative.get('id'),
                    'category1': representative.get('category1', ''),
                    'category2': representative.get('category2', ''),
                    'question': representative.get('question', ''),
                    'representativeAnswer': representative.get('answer', ''),
                    'similarityCount': len(current_group),
                    'avgSimilarity': representative['avgSimilarity'],
                    'removedAnswers': removed_answers
                })
            
            # question ì†ì„±ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
            if 'question' not in representative:
                representative['question'] = ''
            
            unique_answers.append(representative)
            processed_indices.add(i)
        
        # similarityCount -> category1 -> category2 ìˆœìœ¼ë¡œ ì •ë ¬
        unique_answers.sort(key=lambda x: (
            -x.get('similarityCount', 0),  # similarityCount ë‚´ë¦¼ì°¨ìˆœ
            x.get('category1', ''),        # category1 ì˜¤ë¦„ì°¨ìˆœ
            x.get('category2', '')         # category2 ì˜¤ë¦„ì°¨ìˆœ
        ))
        
        similarity_time = time.time() - similarity_start
        total_time = time.time() - start_time
        
        print(f"ğŸ“Š Total comparisons made: {total_comparisons:,}")
        print(f"â±ï¸ Similarity grouping completed in {similarity_time:.2f} seconds")
        print(f"â±ï¸ Total similarity detection time: {total_time:.2f} seconds")
        
        return unique_answers, similar_groups
    
    def process_and_save(self) -> None:
        """ë‹µë³€ í•„í„°ë§ í›„ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ë° íŒŒì¼ ì €ì¥"""
        total_start_time = time.time()
        print('ğŸš€ Starting answer filtering and similarity-based deduplication...')
        
        # 1ë‹¨ê³„: ë‹µë³€ í•„í„°ë§ (ìëª¨ ë‚˜ì—´, ìˆ«ìê°œ ë“± ì œê±°)
        step1_start = time.time()
        print('ğŸ“‹ Step 1: Filtering meaningless answers...')
        filtered_answers, removed_answers = self.answer_filter.run()
        step1_time = time.time() - step1_start
        
        print(f'ğŸ“Š Filtered out {len(removed_answers)} meaningless answers')
        print(f'ğŸ“Š Proceeding with {len(filtered_answers)} valid answers')
        print(f'â±ï¸ Step 1 completed in {step1_time:.2f} seconds')
        
        # 2ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        step2_start = time.time()
        print('ğŸ” Step 2: Removing similar duplicates...')
        unique_answers, similar_groups = self.find_similar_groups(filtered_answers)
        step2_time = time.time() - step2_start
        print(f'â±ï¸ Step 2 completed in {step2_time:.2f} seconds')
        
        # similar_groupsë„ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        similar_groups.sort(key=lambda x: (
            -x.get('similarityCount', 0),  # similarityCount ë‚´ë¦¼ì°¨ìˆœ
            x.get('category1', ''),        # category1 ì˜¤ë¦„ì°¨ìˆœ
            x.get('category2', '')         # category2 ì˜¤ë¦„ì°¨ìˆœ
        ))
        
        # similarityCountê°€ 3ì¸ ë°ì´í„° ë³„ë„ ì¶”ì¶œ (category1, category2ë¡œë§Œ ì •ë ¬)
        similarity_count_3 = [answer for answer in unique_answers if answer.get('similarityCount', 0) == 3]
        similarity_count_3.sort(key=lambda x: (
            x.get('category1', ''),        # category1 ì˜¤ë¦„ì°¨ìˆœ
            x.get('category2', '')         # category2 ì˜¤ë¦„ì°¨ìˆœ
        ))
        
        # ê²°ê³¼ ì €ì¥
        save_start = time.time()
        self.save_json_file(unique_answers, 'answers_similarity_unique.json')
        self.save_json_file(similar_groups, 'answers_similarity_removed.json')
        self.save_json_file(similarity_count_3, 'answers_similarity_count_3.json')
        save_time = time.time() - save_start
        print(f'â±ï¸ File saving completed in {save_time:.2f} seconds')
        
        # í†µê³„ ì¶œë ¥
        original_count = len(self.load_answers())  # ì›ë³¸ ë°ì´í„° ê°œìˆ˜
        filtered_count = len(filtered_answers)
        meaningless_removed = len(removed_answers)
        similarity_removed = sum(len(group['removedAnswers']) for group in similar_groups)
        final_count = len(unique_answers)
        
        total_removal_rate = ((meaningless_removed + similarity_removed) / original_count) * 100 if original_count > 0 else 0
        meaningless_removal_rate = (meaningless_removed / original_count) * 100 if original_count > 0 else 0
        similarity_removal_rate = (similarity_removed / filtered_count) * 100 if filtered_count > 0 else 0
        similarity_group_rate = (len(similar_groups) / len(unique_answers)) * 100 if len(unique_answers) > 0 else 0
        
        print(f'\nğŸ“Š === FINAL STATISTICS ===')
        print(f'âœ¨ Original answers: {original_count}')
        print(f'âœ¨ Meaningless answers removed: {meaningless_removed} ({meaningless_removal_rate:.2f}%)')
        print(f'âœ¨ Filtered answers: {filtered_count}')
        print(f'âœ¨ Similar answers removed: {similarity_removed} ({similarity_removal_rate:.2f}%)')
        print(f'âœ¨ Final unique answers: {final_count}')
        print(f'âœ¨ Similarity groups: {len(similar_groups)}')
        print(f'âœ¨ Similarity count 3 answers: {len(similarity_count_3)}')
        print(f'ğŸ“Š Total removal rate: {total_removal_rate:.2f}%')
        print(f'ğŸ“Š Similarity group rate: {similarity_group_rate:.2f}%')
        print(f'ğŸ“Š Math check: {original_count} - {meaningless_removed} - {similarity_removed} = {original_count - meaningless_removed - similarity_removed} (should equal {final_count})')
        
        # ì„ê³„ê°’ë³„ í†µê³„
        if similar_groups:
            avg_similarity = sum(group['avgSimilarity'] for group in similar_groups) / len(similar_groups)
            print(f'ğŸ“Š Average similarity in groups: {avg_similarity:.3f}')
        
        # ì „ì²´ ì²˜ë¦¬ì‹œê°„
        total_time = time.time() - total_start_time
        print(f'\nâ±ï¸ === PROCESSING TIME SUMMARY ===')
        print(f'â±ï¸ Step 1 (Filtering): {step1_time:.2f} seconds')
        print(f'â±ï¸ Step 2 (Similarity): {step2_time:.2f} seconds')
        print(f'â±ï¸ File Saving: {save_time:.2f} seconds')
        print(f'â±ï¸ Total Processing Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)')
    
    def run(self) -> None:
        """ë‹µë³€ í•„í„°ë§ ë° ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì‹¤í–‰"""
        # ë¡œê·¸ ìº¡ì²˜ ì‹œì‘
        sys.stdout = self.log_capture
        
        try:
            start_time = time.time()
            start_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f'ğŸš€ Starting Answer Filtering and Similarity-based Duplicate Removal at {start_datetime}')
            
            self.process_and_save()
            
            end_time = time.time()
            end_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            total_runtime = end_time - start_time
            
            print(f'âœ… Answer filtering and similarity-based deduplication completed successfully!')
            print(f'ğŸ Process finished at {end_datetime}')
            print(f'â±ï¸ Total Runtime: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)')
            
        except Exception as error:
            error_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f'âŒ Answer filtering and similarity-based deduplication failed at {error_time}: {error}')
            raise
        finally:
            # ë¡œê·¸ ìº¡ì²˜ ì¢…ë£Œ ë° ì €ì¥
            sys.stdout = self.log_capture.original_stdout
            self.log_capture.save_logs()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë‹µë³€ í•„í„°ë§ + TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°')
    parser.add_argument('--input', '-i', default='data/answers.json', help='ì…ë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', default='data', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--threshold', '-t', type=float, default=0.8, help='ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)')
    
    args = parser.parse_args()
    
    deduplicator = SimilarityDeduplicator(
        input_file=args.input,
        output_dir=args.output,
        threshold=args.threshold
    )
    deduplicator.run()


if __name__ == "__main__":
    main()